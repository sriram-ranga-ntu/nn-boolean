{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0], [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0], [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0], [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0], [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, 1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, 1.0, -1.0, -1.0, 1.0]]\n"
     ]
    }
   ],
   "source": [
    "f = open(\"had64.txt\")\n",
    "\n",
    "H64 = []\n",
    "for i in range(64):\n",
    "    H64.append([1.0 if i=='+' else -1.0 for i in list(f.readline())[:-1]])\n",
    "    \n",
    "print(H64)\n",
    "\n",
    "# print([[H64[i][j]-H64[j][i] for i in range(64)] for j in range(64)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Permutation of input numbers i.e. truth tables: \n",
      " tensor([175, 185, 179,  15, 143,  75, 229, 207, 227, 113, 154,  66,  20, 213,\n",
      "        180,  84, 114, 169, 132, 151,  46,  85, 152, 149, 250, 189, 150, 147,\n",
      "        247,  27, 217,  60,  54, 253,  19,  99, 237, 112, 203, 246, 239, 181,\n",
      "        199,   2, 214, 108, 224,  88, 100,  74,  51,  69, 252, 188, 200,  76,\n",
      "         33, 202, 233, 170, 235,  39,  43, 212,  81, 172, 140,  53, 194,  52,\n",
      "         12, 238, 109, 125, 166, 232,  63, 110, 138, 103, 167,  87, 111, 131,\n",
      "         83, 216,  91, 223, 141, 178,  16, 206, 168,  22, 197,   9,   0, 174,\n",
      "         57, 243,  94,  44,  42, 244,  59, 124,  30, 191, 231, 164,  61,  14,\n",
      "         21, 218, 142,   3,   4, 145, 225, 102,  49, 118, 190, 193, 209,  36,\n",
      "        129, 245, 123,  28, 236, 130, 162, 155,  62,  17, 176, 230, 177, 226,\n",
      "        128,   7,  86, 139,  73, 201,  89,  29, 228,   6, 104,  72, 106, 117,\n",
      "        163, 156,  23,  34,  65, 186,  48, 160,  11,  96, 234, 242, 220,  58,\n",
      "         25,  92, 126,  97, 184, 221, 171,  45,  50, 208,  79, 153, 122, 134,\n",
      "          8, 241, 249, 205, 198,  37, 116, 173, 251,  71, 133, 195, 211,  64,\n",
      "         40,  41, 165,  70,  55, 159,  93,   5,  35, 240,  98, 144, 127, 135,\n",
      "         77, 121, 148, 158, 105, 146,  26,  31, 192, 210,  95,  67, 215, 219,\n",
      "        101, 248, 136,   1, 115,  82, 137, 187,  10, 157, 120, 204,  56,  78,\n",
      "         24, 107,  80, 182,  18,  13,  38, 196,  47, 183,  90,  32, 255, 161,\n",
      "        119,  68, 222, 254])\n",
      "Rank is  8\n",
      "Binary version of permutation: \n",
      " [[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0], [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0], [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0], [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0], [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0], [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0], [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0], [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0], [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0], [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0], [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0], [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0], [1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0], [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0], [0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0], [1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0], [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0], [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0], [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0], [0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0], [0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0], [1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0], [0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0], [0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0], [1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=  99 \n",
      " tensor([[ 1.1771,  0.4317,  0.2672,  0.5434,  0.3469,  1.0017,  0.4120,  0.4333],\n",
      "        [ 0.7274, -1.0886,  0.7442, -0.8499,  0.5211, -0.5253,  1.2004, -0.9996],\n",
      "        [ 0.9458,  0.2925, -0.7036,  0.0436,  0.9286,  1.1677, -0.6119, -0.4789],\n",
      "        [ 0.7012, -0.0601, -1.1270,  0.5856,  1.1878, -0.7329, -0.1760,  0.3402],\n",
      "        [ 0.9251,  0.3738,  0.3445,  0.8533, -1.1675, -0.6212, -0.3157, -0.4664],\n",
      "        [ 0.8507, -0.2473,  0.9041, -0.8743, -0.5874,  0.8404, -0.2157,  0.5961],\n",
      "        [ 1.0661,  0.5662, -0.5164, -0.2590, -1.0208, -0.6494,  0.7053,  0.3094],\n",
      "        [ 1.1640, -0.0729, -0.2403,  0.3253, -0.4266,  0.2249,  0.1824, -0.2666]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  199 \n",
      " tensor([[ 1.1426,  0.6635,  0.6122,  0.7205,  0.6133,  1.0139,  0.5865,  0.7947],\n",
      "        [ 0.8782, -1.1264,  0.8395, -0.8371,  0.8957, -0.7413,  1.3447, -1.2047],\n",
      "        [ 0.9563,  0.5450, -0.8749, -0.3047,  0.9373,  1.2414, -0.7287, -0.7912],\n",
      "        [ 0.7371, -0.5262, -1.1759,  0.8383,  1.1915, -0.7743, -0.2170,  0.4693],\n",
      "        [ 0.9334,  0.6173,  0.6708,  1.1478, -1.2181, -0.7547, -0.6768, -0.8918],\n",
      "        [ 0.8202, -0.5897,  0.9737, -0.9567, -0.7555,  1.1132, -0.4034,  0.7318],\n",
      "        [ 1.0025,  0.6626, -0.9409, -0.5977, -1.1509, -0.7615,  1.0729,  0.6860],\n",
      "        [ 1.1845, -0.2923, -0.4308,  0.3971, -0.5478,  0.2744,  0.1803, -0.4071]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  299 \n",
      " tensor([[ 1.1080,  0.8572,  0.8656,  0.8572,  0.8147,  0.9378,  0.7172,  1.0072],\n",
      "        [ 0.9108, -1.1257,  0.8668, -0.8394,  0.9266, -0.7840,  1.3213, -1.1930],\n",
      "        [ 0.9569,  0.7338, -1.0154, -0.5911,  0.8897,  1.1935, -0.7908, -1.0021],\n",
      "        [ 0.7739, -0.8749, -1.1635,  1.0258,  1.1112, -0.6988, -0.3110,  0.5852],\n",
      "        [ 0.9508,  0.7705,  0.8441,  1.1976, -1.1536, -0.8121, -0.7897, -1.0712],\n",
      "        [ 0.8319, -0.8689,  0.9310, -0.9183, -0.8732,  1.2036, -0.4979,  0.7469],\n",
      "        [ 0.9719,  0.7764, -1.0847, -0.7674, -1.1340, -0.8123,  1.1796,  0.8463],\n",
      "        [ 1.1931, -0.4776, -0.5688,  0.4586, -0.6609,  0.3795,  0.2066, -0.4961]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  399 \n",
      " tensor([[ 1.0798,  0.9857,  1.0016,  0.9185,  0.9410,  0.9031,  0.7905,  1.0879],\n",
      "        [ 0.9189, -1.1153,  0.8880, -0.8501,  0.9358, -0.8115,  1.2822, -1.1676],\n",
      "        [ 0.9564,  0.8518, -1.0745, -0.7770,  0.9041,  1.1493, -0.8186, -1.0864],\n",
      "        [ 0.8103, -1.0786, -1.1620,  1.1512,  1.0130, -0.6830, -0.4236,  0.6654],\n",
      "        [ 0.9556,  0.8661,  0.9028,  1.1566, -1.0972, -0.8567, -0.8181, -1.1052],\n",
      "        [ 0.8601, -1.0408,  0.8964, -0.8731, -0.9726,  1.2261, -0.5852,  0.7698],\n",
      "        [ 0.9589,  0.8659, -1.0942, -0.8461, -1.0944, -0.8560,  1.1808,  0.8876],\n",
      "        [ 1.1916, -0.6219, -0.6696,  0.5404, -0.7563,  0.4882,  0.2827, -0.5666]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  499 \n",
      " tensor([[ 1.0583,  1.0410,  1.0458,  0.9295,  1.0025,  0.9068,  0.8375,  1.0921],\n",
      "        [ 0.9282, -1.0966,  0.9066, -0.8700,  0.9446, -0.8474,  1.2343, -1.1409],\n",
      "        [ 0.9565,  0.9139, -1.0715, -0.8747,  0.9390,  1.1138, -0.8436, -1.0929],\n",
      "        [ 0.8471, -1.1478, -1.1602,  1.2028,  0.9448, -0.7245, -0.5372,  0.7259],\n",
      "        [ 0.9566,  0.9193,  0.9295,  1.1093, -1.0578, -0.8911, -0.8440, -1.0945],\n",
      "        [ 0.8886, -1.1039,  0.8868, -0.8512, -1.0345,  1.1999, -0.6670,  0.8048],\n",
      "        [ 0.9566,  0.9188, -1.0723, -0.8931, -1.0578, -0.8904,  1.1565,  0.9048],\n",
      "        [ 1.1784, -0.7267, -0.7443,  0.6338, -0.8268,  0.5965,  0.3956, -0.6355]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  599 \n",
      " tensor([[ 1.0427,  1.0452,  1.0474,  0.9340,  1.0223,  0.9284,  0.8766,  1.0739],\n",
      "        [ 0.9406, -1.0723,  0.9263, -0.8962,  0.9552, -0.8881,  1.1820, -1.1106],\n",
      "        [ 0.9606,  0.9451, -1.0522, -0.9213,  0.9634,  1.0819, -0.8759, -1.0751],\n",
      "        [ 0.8813, -1.1321, -1.1396,  1.1917,  0.9254, -0.7938, -0.6462,  0.7858],\n",
      "        [ 0.9611,  0.9477,  0.9489,  1.0755, -1.0352, -0.9216, -0.8770, -1.0748],\n",
      "        [ 0.9141, -1.0962,  0.9000, -0.8618, -1.0530,  1.1504, -0.7448,  0.8465],\n",
      "        [ 0.9609,  0.9473, -1.0522, -0.9252, -1.0355, -0.9209,  1.1238,  0.9246],\n",
      "        [ 1.1522, -0.8069, -0.8065,  0.7234, -0.8762,  0.7063,  0.5279, -0.7136]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  699 \n",
      " tensor([[ 1.0307,  1.0311,  1.0358,  0.9486,  1.0208,  0.9534,  0.9126,  1.0524],\n",
      "        [ 0.9553, -1.0467,  0.9477, -0.9253,  0.9676, -0.9282,  1.1296, -1.0780],\n",
      "        [ 0.9693,  0.9658, -1.0352, -0.9480,  0.9768,  1.0522, -0.9110, -1.0529],\n",
      "        [ 0.9131, -1.0880, -1.1026,  1.1456,  0.9389, -0.8668, -0.7494,  0.8487],\n",
      "        [ 0.9698,  0.9673,  0.9650,  1.0513, -1.0228, -0.9499, -0.9120, -1.0527],\n",
      "        [ 0.9371, -1.0645,  0.9264, -0.8951, -1.0443,  1.0973, -0.8188,  0.8914],\n",
      "        [ 0.9696,  0.9669, -1.0357, -0.9488, -1.0230, -0.9493,  1.0888,  0.9467],\n",
      "        [ 1.1157, -0.8771, -0.8649,  0.8056, -0.9144,  0.8121,  0.6635, -0.7982]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  799 \n",
      " tensor([[ 1.0199,  1.0164,  1.0219,  0.9675,  1.0132,  0.9752,  0.9448,  1.0318],\n",
      "        [ 0.9709, -1.0242,  0.9687, -0.9538,  0.9802, -0.9623,  1.0816, -1.0469],\n",
      "        [ 0.9800,  0.9825, -1.0208, -0.9686,  0.9862,  1.0272, -0.9440, -1.0317],\n",
      "        [ 0.9435, -1.0459, -1.0621,  1.0911,  0.9618, -0.9295, -0.8423,  0.9086],\n",
      "        [ 0.9803,  0.9832,  0.9792,  1.0313, -1.0136, -0.9737, -0.9446, -1.0317],\n",
      "        [ 0.9592, -1.0337,  0.9554, -0.9343, -1.0277,  1.0516, -0.8858,  0.9342],\n",
      "        [ 0.9801,  0.9828, -1.0213, -0.9686, -1.0138, -0.9733,  1.0561,  0.9678],\n",
      "        [ 1.0753, -0.9366, -0.9194,  0.8807, -0.9482,  0.9015,  0.7884, -0.8788]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  899 \n",
      " tensor([[ 1.0105,  1.0061,  1.0102,  0.9839,  1.0063,  0.9904,  0.9711,  1.0152],\n",
      "        [ 0.9849, -1.0088,  0.9856, -0.9774,  0.9907, -0.9856,  1.0424, -1.0221],\n",
      "        [ 0.9897,  0.9937, -1.0094, -0.9848,  0.9936,  1.0102, -0.9710, -1.0147],\n",
      "        [ 0.9704, -1.0169, -1.0288,  1.0448,  0.9819, -0.9729, -0.9177,  0.9567],\n",
      "        [ 0.9898,  0.9939,  0.9905,  1.0152, -1.0064, -0.9900, -0.9712, -1.0148],\n",
      "        [ 0.9786, -1.0124,  0.9793, -0.9676, -1.0132,  1.0199, -0.9403,  0.9688],\n",
      "        [ 0.9896,  0.9937, -1.0098, -0.9846, -1.0065, -0.9898,  1.0293,  0.9848],\n",
      "        [ 1.0391, -0.9770, -0.9631,  0.9420, -0.9758,  0.9626,  0.8902, -0.9433]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  999 \n",
      " tensor([[ 1.0039,  1.0013,  1.0031,  0.9945,  1.0020,  0.9977,  0.9888,  1.0049],\n",
      "        [ 0.9945, -1.0018,  0.9957, -0.9925,  0.9971, -0.9967,  1.0162, -1.0069],\n",
      "        [ 0.9963,  0.9987, -1.0027, -0.9950,  0.9981,  1.0023, -0.9891, -1.0045],\n",
      "        [ 0.9891, -1.0035, -1.0087,  1.0151,  0.9944, -0.9936, -0.9684,  0.9862],\n",
      "        [ 0.9963,  0.9988,  0.9972,  1.0050, -1.0019, -0.9977, -0.9891, -1.0046],\n",
      "        [ 0.9921, -1.0026,  0.9937, -0.9891, -1.0041,  1.0047, -0.9770,  0.9900],\n",
      "        [ 0.9962,  0.9987, -1.0029, -0.9949, -1.0020, -0.9977,  1.0112,  0.9952],\n",
      "        [ 1.0142, -0.9954, -0.9891,  0.9809, -0.9926,  0.9915,  0.9584, -0.9823]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1099 \n",
      " tensor([[ 1.0008,  0.9987,  1.0005,  0.9990,  1.0003,  0.9997,  0.9974,  1.0008],\n",
      "        [ 0.9989, -0.9902,  0.9994, -0.9987,  0.9996, -0.9996,  1.0036, -1.0011],\n",
      "        [ 0.9993,  0.9943, -1.0004, -0.9992,  0.9997,  1.0002, -0.9976, -1.0007],\n",
      "        [ 0.9978, -1.0194, -1.0013,  1.0026,  0.9991, -0.9993, -0.9929,  0.9978],\n",
      "        [ 0.9993,  1.0010,  0.9996,  1.0009, -1.0003, -0.9998, -0.9976, -1.0007],\n",
      "        [ 0.9984, -0.9967,  0.9991, -0.9981, -1.0006,  1.0005, -0.9948,  0.9984],\n",
      "        [ 0.9992,  1.0013, -1.0004, -0.9991, -1.0003, -0.9997,  1.0025,  0.9993],\n",
      "        [ 1.0028, -0.9994, -0.9985,  0.9968, -0.9989,  0.9991,  0.9909, -0.9973]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1199 \n",
      " tensor([[ 1.0001,  1.0004,  1.0001,  0.9999,  1.0002,  1.0000,  0.9998,  1.0001],\n",
      "        [ 0.9999, -1.0049,  0.9998, -0.9999,  0.9997, -0.9999,  1.0003, -1.0001],\n",
      "        [ 1.0000,  1.0030, -0.9999, -1.0000,  0.9998,  1.0000, -0.9998, -1.0000],\n",
      "        [ 0.9998, -0.9935, -0.9999,  1.0001,  0.9994, -0.9999, -0.9994,  0.9999],\n",
      "        [ 1.0000,  0.9983,  1.0002,  1.0000, -1.0002, -1.0000, -0.9998, -1.0000],\n",
      "        [ 0.9999, -1.0030,  1.0000, -0.9999, -1.0005,  1.0001, -0.9995,  0.9999],\n",
      "        [ 0.9999,  0.9983, -0.9999, -1.0000, -1.0002, -1.0000,  1.0002,  1.0000],\n",
      "        [ 1.0002, -1.0008, -0.9998,  0.9998, -0.9993,  0.9999,  0.9992, -0.9999]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ep=  1299 \n",
      " tensor([[ 0.9999,  1.0018,  1.0000,  1.0000,  1.0003,  1.0001,  0.9999,  1.0000],\n",
      "        [ 1.0001, -1.0092,  1.0000, -1.0000,  0.9997, -1.0001,  1.0001, -0.9999],\n",
      "        [ 1.0001,  1.0022, -1.0000, -1.0000,  0.9997,  0.9999, -0.9999, -1.0000],\n",
      "        [ 1.0002, -0.9938, -1.0001,  0.9999,  0.9991, -1.0002, -0.9998,  1.0001],\n",
      "        [ 1.0001,  1.0070,  1.0000,  1.0000, -1.0002, -1.0001, -0.9999, -1.0000],\n",
      "        [ 1.0002, -0.9979,  0.9999, -1.0000, -1.0005,  0.9998, -0.9999,  1.0001],\n",
      "        [ 1.0001,  1.0076, -1.0000, -1.0000, -1.0002, -1.0001,  1.0001,  1.0000],\n",
      "        [ 0.9997, -0.9955, -0.9999,  1.0001, -0.9992,  1.0002,  0.9998, -1.0001]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1399 \n",
      " tensor([[ 1.0001,  1.0001,  1.0000,  0.9999,  1.0008,  0.9999,  1.0000,  1.0000],\n",
      "        [ 0.9999, -1.0004,  1.0000, -0.9999,  0.9939, -0.9999,  1.0001, -1.0000],\n",
      "        [ 0.9999,  1.0001, -1.0000, -0.9999,  1.0033,  1.0000, -1.0000, -1.0000],\n",
      "        [ 0.9998, -0.9998, -0.9999,  1.0003,  1.0048, -0.9999, -0.9999,  1.0000],\n",
      "        [ 0.9999,  1.0002,  1.0000,  1.0001, -1.0024, -1.0000, -1.0000, -1.0000],\n",
      "        [ 0.9999, -1.0000,  1.0001, -0.9998, -1.0052,  1.0001, -0.9999,  1.0000],\n",
      "        [ 0.9999,  1.0002, -1.0000, -0.9999, -1.0024, -1.0000,  1.0000,  1.0000],\n",
      "        [ 1.0002, -0.9998, -1.0001,  0.9997, -0.9999,  0.9998,  0.9998, -1.0000]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1499 \n",
      " tensor([[ 0.9998,  1.0000,  0.9999,  1.0000,  0.9966,  1.0000,  1.0001,  1.0001],\n",
      "        [ 1.0002, -1.0000,  1.0001, -0.9999,  1.0022, -0.9999,  0.9998, -1.0001],\n",
      "        [ 1.0001,  1.0000, -1.0000, -1.0000,  0.9945,  1.0000, -1.0001, -1.0000],\n",
      "        [ 1.0004, -0.9999, -0.9999,  1.0001,  0.9951, -0.9999, -1.0003,  0.9998],\n",
      "        [ 1.0001,  1.0000,  1.0000,  1.0000, -1.0011, -1.0000, -1.0001, -1.0000],\n",
      "        [ 1.0003, -1.0000,  1.0001, -0.9999, -0.9995,  1.0001, -1.0002,  0.9999],\n",
      "        [ 1.0001,  1.0000, -1.0000, -1.0000, -1.0000, -1.0000,  0.9999,  0.9999],\n",
      "        [ 0.9995, -1.0001, -1.0002,  0.9998, -1.0012,  0.9999,  1.0004, -0.9998]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1599 \n",
      " tensor([[ 0.9998,  1.0000,  1.0000,  1.0000,  0.9999,  0.9995,  1.0001,  1.0001],\n",
      "        [ 1.0003, -1.0000,  1.0000, -1.0000,  1.0002, -0.9993,  0.9999, -1.0002],\n",
      "        [ 1.0002,  1.0000, -1.0000, -1.0000,  0.9999,  1.0001, -1.0001, -1.0001],\n",
      "        [ 1.0006, -1.0000, -1.0000,  1.0001,  0.9999, -0.9991, -1.0002,  0.9997],\n",
      "        [ 1.0002,  1.0000,  1.0000,  1.0000, -1.0001, -0.9996, -1.0001, -1.0001],\n",
      "        [ 1.0005, -1.0000,  1.0000, -1.0000, -1.0000,  1.0010, -1.0001,  0.9998],\n",
      "        [ 1.0002,  1.0000, -1.0000, -1.0000, -1.0001, -0.9996,  0.9999,  0.9999],\n",
      "        [ 0.9992, -1.0000, -1.0000,  0.9999, -1.0001,  0.9989,  1.0002, -0.9996]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1699 \n",
      " tensor([[ 1.0011,  1.0000,  0.9999,  1.0078,  1.0000,  1.0106,  0.9997,  1.0000],\n",
      "        [ 0.9992, -1.0000,  1.0001, -1.0114,  1.0000, -1.0101,  1.0003, -1.0000],\n",
      "        [ 1.0013,  1.0000, -0.9999, -0.9836,  1.0000,  1.0166, -0.9997, -1.0000],\n",
      "        [ 1.0007, -1.0001, -0.9998,  1.0232,  0.9999, -0.9825, -0.9990,  1.0001],\n",
      "        [ 1.0001,  1.0000,  1.0001,  0.9998, -1.0000, -0.9935, -0.9999, -1.0000],\n",
      "        [ 0.9995, -1.0001,  1.0001, -1.0051, -1.0001,  0.9999, -0.9995,  1.0001],\n",
      "        [ 1.0000,  1.0000, -0.9999, -1.0016, -1.0000, -0.9960,  1.0001,  1.0000],\n",
      "        [ 1.0009, -0.9999, -1.0002,  1.0028, -0.9999,  1.0061,  0.9992, -1.0001]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1799 \n",
      " tensor([[ 1.0146,  1.0001,  1.0001,  1.0002,  1.0000,  0.9998,  0.9830,  0.9997],\n",
      "        [ 0.9987, -1.0001,  0.9998, -0.9997,  1.0000, -0.9999,  1.0071, -0.9996],\n",
      "        [ 1.0222,  0.9999, -1.0001, -0.9996,  1.0000,  0.9997, -1.0273, -0.9998],\n",
      "        [ 1.0171, -1.0002, -1.0004,  1.0002,  1.0001, -1.0002, -1.0254,  1.0009],\n",
      "        [ 0.9998,  0.9999,  0.9999,  0.9997, -1.0000, -1.0000, -1.0033, -0.9997],\n",
      "        [ 0.9974, -1.0002,  0.9997, -1.0002, -0.9999,  1.0001, -0.9978,  1.0007],\n",
      "        [ 0.9954,  0.9999, -1.0001, -1.0005, -1.0000, -1.0000,  1.0003,  1.0003],\n",
      "        [ 1.0041, -0.9997, -0.9995,  0.9998, -1.0001,  0.9999,  0.9921, -1.0010]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1899 \n",
      " tensor([[ 0.9985,  0.9999,  0.9997,  1.0000,  1.0001,  1.0000,  0.9984,  0.9984],\n",
      "        [ 1.0007, -0.9999,  1.0004, -1.0000,  0.9999, -1.0000,  1.0012, -0.9920],\n",
      "        [ 0.9976,  1.0001, -0.9997, -1.0000,  1.0000,  1.0000, -1.0025, -1.0051],\n",
      "        [ 0.9979, -0.9998, -0.9991,  1.0001,  0.9998, -0.9999, -1.0024,  0.9914],\n",
      "        [ 0.9998,  1.0001,  1.0002,  1.0000, -1.0001, -1.0000, -1.0007, -0.9982],\n",
      "        [ 1.0003, -0.9999,  1.0006, -0.9999, -1.0001,  1.0000, -0.9999,  1.0047],\n",
      "        [ 1.0003,  1.0001, -0.9997, -1.0000, -1.0001, -1.0000,  0.9997,  1.0019],\n",
      "        [ 0.9996, -1.0002, -1.0010,  0.9999, -0.9998,  0.9999,  0.9993, -1.0001]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "ep=  1999 \n",
      " tensor([[ 1.0001,  0.9998,  1.0004,  1.0000,  1.0000,  1.0000,  0.9999,  1.0024],\n",
      "        [ 1.0000, -0.9997,  0.9983, -1.0000,  1.0000, -1.0000,  1.0001, -0.9991],\n",
      "        [ 1.0002,  1.0002, -0.9989, -1.0000,  1.0000,  1.0000, -1.0001, -0.9960],\n",
      "        [ 1.0002, -0.9994, -0.9987,  0.9999,  1.0000, -0.9999, -1.0001,  1.0024],\n",
      "        [ 1.0000,  1.0002,  1.0002,  1.0000, -1.0000, -1.0000, -1.0000, -1.0015],\n",
      "        [ 1.0000, -0.9996,  0.9991, -1.0000, -1.0000,  1.0000, -1.0000,  0.9987],\n",
      "        [ 1.0000,  1.0002, -1.0000, -1.0000, -1.0000, -1.0000,  1.0000,  0.9973],\n",
      "        [ 1.0000, -1.0007, -0.9998,  1.0001, -0.9999,  0.9999,  1.0000, -1.0005]],\n",
      "       grad_fn=<PermuteBackward0>) 6\n",
      "Final rounded weight function: \n",
      " tensor([[ 1.,  1.,  1.,  1.,  1.,  1.,  1.,  1.],\n",
      "        [ 1., -1.,  1., -1.,  1., -1.,  1., -1.],\n",
      "        [ 1.,  1., -1., -1.,  1.,  1., -1., -1.],\n",
      "        [ 1., -1., -1.,  1.,  1., -1., -1.,  1.],\n",
      "        [ 1.,  1.,  1.,  1., -1., -1., -1., -1.],\n",
      "        [ 1., -1.,  1., -1., -1.,  1., -1.,  1.],\n",
      "        [ 1.,  1., -1., -1., -1., -1.,  1.,  1.],\n",
      "        [ 1., -1., -1.,  1., -1.,  1.,  1., -1.]], grad_fn=<RoundBackward0>)\n",
      "Test:\n",
      "tensor([ 5.,  1., -1., -1.,  1.,  1.,  3., -1.]) \n",
      " tensor([ 5.,  1., -1., -1.,  1.,  1.,  3., -1.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 4., -2., -2.,  0.,  2.,  0.,  0., -2.]) \n",
      " tensor([ 4., -2., -2., -0.,  2.,  0., -0., -2.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 4.,  2.,  0.,  2.,  0., -2.,  0.,  2.]) \n",
      " tensor([ 4.,  2.,  0.,  2., -0., -2., -0.,  2.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 2.,  0.,  0., -2.,  0., -2.,  2.,  0.]) \n",
      " tensor([ 2.,  0., -0., -2., -0., -2.,  2., -0.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 2., -2.,  0.,  0.,  0.,  0., -2.,  2.]) \n",
      " tensor([ 2., -2.,  0., -0., -0.,  0., -2.,  2.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 5., -3.,  1.,  1.,  1.,  1.,  1.,  1.]) \n",
      " tensor([ 5., -3.,  1.,  1.,  1.,  1.,  1.,  1.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 4.,  0.,  0.,  0.,  2.,  2., -2.,  2.]) \n",
      " tensor([ 4.,  0.,  0., -0.,  2.,  2., -2.,  2.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 3., -3.,  1., -1.,  1., -1., -1.,  1.]) \n",
      " tensor([ 3., -3.,  1., -1.,  1., -1., -1.,  1.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 4.,  0., -2., -2.,  2., -2.,  0.,  0.]) \n",
      " tensor([ 4.,  0., -2., -2.,  2., -2., -0.,  0.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n",
      "tensor([ 4.,  2.,  0.,  2.,  0.,  2.,  0., -2.]) \n",
      " tensor([ 4.,  2.,  0.,  2.,  0.,  2., -0., -2.], grad_fn=<RoundBackward0>) \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "\n",
    "    def __init__(self, inputDim):\n",
    "\n",
    "        # Initialize the network layers.\n",
    "\n",
    "        super(NetworkModel, self).__init__()\n",
    "\n",
    "        self.lin1 = nn.Linear(inputDim, inputDim, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # A forward function\n",
    "        # Linear function without activation\n",
    "\n",
    "        x = self.lin1(x)\n",
    "        \n",
    "#         F-> WalshSpec\n",
    "#         WalshSpec = F * W_t\n",
    "\n",
    "        return x\n",
    "\n",
    "class TrainModel():\n",
    "\n",
    "    def __init__(self, model, device, learningRate, inputDim, batchSize, numberOfSteps, epochs):\n",
    "\n",
    "        self.device = device\n",
    "        self.net = model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learningRate)\n",
    "        self.inputDim = inputDim\n",
    "        self.batchSize = batchSize\n",
    "        self.numberOfSteps = numberOfSteps\n",
    "        self.epochs = epochs\n",
    "        self.H = {}\n",
    "        self.H[4] = torch.tensor([[1.0, 1.0, 1.0, 1.0], [1.0, -1.0, 1.0, -1.0], [1.0, 1.0, -1.0, -1.0], [1.0, -1.0, -1.0, 1.0]])\n",
    "        self.H[8] = torch.tensor([[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0],\n",
    "                            [1.0, -1.0, 1.0, -1.0, 1.0, -1.0, 1.0, -1.0],\n",
    "                            [1.0, 1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0],\n",
    "                            [1.0, -1.0, -1.0, 1.0, 1.0, -1.0, -1.0, 1.0],\n",
    "                            [1.0, 1.0, 1.0, 1.0, -1.0, -1.0, -1.0, -1.0],\n",
    "                            [1.0, -1.0, 1.0, -1.0, -1.0, 1.0, -1.0, 1.0],\n",
    "                            [1.0, 1.0, -1.0, -1.0, -1.0, -1.0, 1.0, 1.0],\n",
    "                            [1.0, -1.0, -1.0, 1.0, -1.0, 1.0, 1.0, -1.0]])\n",
    "        self.H[64] = H64\n",
    "        if(self.inputDim<20):\n",
    "#             For generating data of insuficcient LI inputs:\n",
    "#             while(True):\n",
    "#                 self.perm = torch.randperm(pow(2, self.inputDim))\n",
    "# #                 print(\"Permutation of input numbers i.e. truth tables: \\n\", self.perm)\n",
    "#                 self.data = [[0.0]*(self.inputDim-len(bin(num)[2:]))+[float(i) for i in bin(num)[2:]] for num in self.perm]\n",
    "#                 if(np.linalg.matrix_rank(self.data[:self.numberOfSteps])>=self.inputDim):\n",
    "#                     print(\"Rank not small enough, generating data again\")\n",
    "#                 else:\n",
    "#                     break\n",
    "            \n",
    "            self.perm = torch.randperm(pow(2, self.inputDim))\n",
    "            print(\"Permutation of input numbers i.e. truth tables: \\n\", self.perm)\n",
    "            self.data = [[0.0]*(self.inputDim-len(bin(num)[2:]))+[float(i) for i in bin(num)[2:]] for num in self.perm]\n",
    "            print(\"Rank is \", np.linalg.matrix_rank(self.data[:self.numberOfSteps]))\n",
    "            print(\"Binary version of permutation: \\n\", self.data)\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            print(0)\n",
    "            \n",
    "    \n",
    "    def train(self, ):\n",
    "        \n",
    "        for ep in range(self.epochs):\n",
    "            \n",
    "            for step in range(self.numberOfSteps):\n",
    "\n",
    "                self.optimizer.zero_grad()\n",
    "                input = torch.tensor(self.data[step])\n",
    "                output = self.net(input)\n",
    "                loss = F.mse_loss(torch.matmul(input, self.H[self.inputDim]), output)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if ((ep+1)%100==0):\n",
    "                weight_dict = OrderedDict(self.net.named_parameters())\n",
    "                weightFunction = weight_dict['lin1.weight'].T\n",
    "                print(\"ep= \", ep, \"\\n\", weightFunction, 6)\n",
    "        \n",
    "        weight_dict = OrderedDict(self.net.named_parameters())\n",
    "        weightFunction = weight_dict['lin1.weight'].T\n",
    "        print(\"Final rounded weight function: \\n\", torch.round(weightFunction))\n",
    "\n",
    "\n",
    "\n",
    "    def test(self,n):\n",
    "\n",
    "#         test_samples = torch.tensor([[float(el) for el in row] for row in torch.randint(0, 2, (n, self.inputDim)).to(self.device)])\n",
    "        test_samples = torch.tensor(self.data[self.numberOfSteps:self.numberOfSteps+n])    \n",
    "        preds = self.net(test_samples)\n",
    "        outputs = torch.matmul(test_samples, self.H[self.inputDim])\n",
    "        print(\"Test:\")\n",
    "        for i in range(len(preds)):\n",
    "            print(outputs[i], \"\\n\", torch.round(preds[i]), \"\\n\\n\")\n",
    "\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Please change the inputs here\n",
    "    inputDim = 8\n",
    "\n",
    "    # Additional inputs may not be required to change.\n",
    "    learningRate = 0.005\n",
    "    batchSize = 1\n",
    "    numberOfSteps = 8\n",
    "    epochs = 2000\n",
    "\n",
    "    # Additional Initializer\n",
    "    device = torch.device('cpu')\n",
    "    model =  NetworkModel(inputDim)\n",
    "    trainer = TrainModel(model, device, learningRate, inputDim, batchSize, numberOfSteps, epochs)\n",
    "    trainer.train()\n",
    "    trainer.test(10)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(np.linalg.matrix_rank([[0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0], [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0], [1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0], [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0], [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0], [1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0], [1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0], [0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0], [0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
