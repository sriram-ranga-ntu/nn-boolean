{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Xzdb3gLgvSdQ",
    "outputId": "9b34134a-fb2a-47f2-e149-77185061c02a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.3468, -0.7553, -1.1756,  1.3883],\n",
      "        [ 0.3677,  1.1636, -1.4343, -0.6734],\n",
      "        [-1.4603,  1.0083,  0.0106,  0.9223],\n",
      "        [-1.2697, -1.0290, -0.7487, -0.8767]], requires_grad=True), Parameter containing:\n",
      "tensor([[ 0.3468,  0.3677, -1.4603, -1.2697],\n",
      "        [-0.7553,  1.1636,  1.0083, -1.0290],\n",
      "        [-1.1756, -1.4343,  0.0106, -0.7487],\n",
      "        [ 1.3883, -0.6734,  0.9223, -0.8767]], requires_grad=True)]\n",
      "H*Ht = \n",
      "tensor([[ 4.0000e+00,  4.7684e-07,  2.3842e-07,  5.9605e-08],\n",
      "        [ 4.7684e-07,  4.0000e+00, -5.9605e-08,  1.1921e-07],\n",
      "        [ 2.3842e-07, -5.9605e-08,  4.0000e+00, -3.5763e-07],\n",
      "        [ 5.9605e-08,  1.1921e-07, -3.5763e-07,  4.0000e+00]],\n",
      "       grad_fn=<MmBackward0>)\n",
      "Results: \n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n",
      "tensor([0.2500, 0.2500, 0.2500, 0.2500], grad_fn=<DivBackward0>)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from collections import OrderedDict\n",
    "\n",
    "class NetworkModel(nn.Module):\n",
    "\n",
    "    def __init__(self, inputDim):\n",
    "\n",
    "        # Initialize the network layers.\n",
    "\n",
    "        super(NetworkModel, self).__init__()\n",
    "\n",
    "        param = torch.randn(inputDim, inputDim)\n",
    "        param_t = torch.transpose(param,0,1)\n",
    "        self.weight1 = torch.nn.Parameter(param)\n",
    "        self.weight1_t = torch.nn.Parameter(param_t)\n",
    "        #self.relu = torch.nn.ReLU()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # A forward function\n",
    "        # Linear function without activation\n",
    "        x = F.linear(x, self.weight1)\n",
    "        #x = self.relu(x)\n",
    "        x = F.linear(x,self.weight1_t)\n",
    "        return x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class TrainModel():\n",
    "\n",
    "    def __init__(self, model, device, learningRate, inputDim, batchSize, numberOfSteps):\n",
    "\n",
    "        self.device = device\n",
    "        self.net = model.to(self.device)\n",
    "        self.optimizer = optim.Adam(self.net.parameters(), lr=learningRate)\n",
    "        self.inputDim = inputDim\n",
    "        self.batchSize = batchSize\n",
    "        self.numberOfSteps = numberOfSteps\n",
    "\n",
    "    def train(self,):\n",
    "\n",
    "        for step in range(self.numberOfSteps):\n",
    "\n",
    "            self.optimizer.zero_grad()\n",
    "            input = torch.randn((self.batchSize, self.inputDim)).to(self.device)\n",
    "            output = (1/self.inputDim)*self.net(input)\n",
    "            loss = F.mse_loss(input, output)\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        parameters = list(self.net.parameters())\n",
    "        mat1 = parameters[0]\n",
    "        mat1_rounded = torch.round(mat1)\n",
    "        mat2 = parameters[1]\n",
    "        mat2_rounded = torch.round(mat2)\n",
    "        mat = mat1 @ mat2\n",
    "        mat_rounded = mat1_rounded @ mat2_rounded\n",
    "        #print(torch.round(weightFunction), 5)\n",
    "        #print(weightFunction2)\n",
    "        print(parameters)\n",
    "        print(\"H*Ht = \")\n",
    "        print(torch.matmul(parameters[0], parameters[1]))\n",
    "        #print(self.net)\n",
    "#         print(torch.round(mat1),2)\n",
    "#         print(torch.round(mat2),3)\n",
    "#         print(torch.round(mat),4)\n",
    "#         print(torch.round(mat_rounded),5)\n",
    "\n",
    "    def test(self,n):\n",
    "\n",
    "        test_samples = torch.randn(n, self.inputDim).to(self.device)\n",
    "        preds = self.net(test_samples)\n",
    "        print(\"Results: \")\n",
    "        for i in range(n):\n",
    "            print(torch.div(test_samples[i], preds[i]))\n",
    "\n",
    "def main():\n",
    "\n",
    "    # Please change the inputs here\n",
    "    inputDim = 4\n",
    "\n",
    "    # Additional inputs may not be required to change.\n",
    "    learningRate = 0.0005\n",
    "    batchSize = 64\n",
    "    numberOfSteps = 10000\n",
    "\n",
    "    # Additional Initializer\n",
    "    device = torch.device('cpu')\n",
    "    model =  NetworkModel(inputDim)\n",
    "    trainer = TrainModel(model, device, learningRate, inputDim, batchSize, numberOfSteps)\n",
    "    trainer.train()\n",
    "    trainer.test(50)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "fWBGDF2BUYCw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[BooleanFunction([[0]], 1), BooleanFunction([[1]], 2), BooleanFunction([[2]], 3), BooleanFunction([[3]], 4), BooleanFunction([[4]], 5)]\n"
     ]
    }
   ],
   "source": [
    "from booleantools import *\n",
    "\n",
    "x = getX(5)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L1H68BLOUZNi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jmJ0vZVlUats"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "17Y6jX_1Vgtv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
